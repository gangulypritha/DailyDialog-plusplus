extends layout

block title     
  title Reasoning over Scientific Plots

block description
  meta(name='description', content='PlotQA: Reasoning over Scientific Plots')

block extralinks
  link(rel='stylesheet', href='/stylesheets/index.css')
  script(async defer src="https://buttons.github.io/buttons.js")

block extrascripts

mixin trained_on_random_negatives(group, is_test)
  table.table.performanceTable
    tr
      if is_test
        th Rank
      th Model
      th Accuracy
    - var human_em = 86.831
    - var human_f1 = 89.452
      tr
        td
          p #{""}
          span.date.label.label-default #{""}
        td
          | Human Baseline 
          p.institution #{"IIT Madras"}
        td #{80.47}

      tr
        td
          p #{"1"}
          span.date.label.label-default #{"March, 2020"}
        td
          | Hybrid Model <b>
          p.institution #{"IIT Madras"}
          a(href="https://arxiv.org/pdf/1909.00997.pdf") (Methani & al. '20)
        td #{22.52}

      tr
        td
          p #{"2"}
          span.date.label.label-default #{"March, 2020"}
        td
          | VOES <b>
          p.institution #{"IIT Madras"}
          a(href="https://arxiv.org/pdf/1909.00997.pdf") (Methani & al. '20)
        td #{18.46}

      tr
        td
          p #{"3"}
          span.date.label.label-default #{"March, 2020"}
        td
          | SAN <b>
          p.institution #{"Carnegie Mellon University"}
          a(href="https://arxiv.org/pdf/1511.02274.pdf") (Yang & al., '16)
        td #{7.76}


mixin trained_on_both(group, is_test)
  table.table.performanceTable
    tr
      if is_test
        th Rank
      th Model
      th Accuracy      
      tr
        td
          p #{"1"}
          span.date.label.label-default #{"March, 2020"}
        td
          | Hybrid Model <b>
          p.institution #{"IIT Madras"}
          a(href="https://arxiv.org/pdf/1909.00997.pdf") (Methani & al. '20)
        td #{57.99}

      tr
        td
          p #{"2"}
          span.date.label.label-default #{"March, 2020"}
        td
          | SANDY-OCR <b>
          p.institution #{"Rochester Institute of Technology"}
          a(href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Kafle_DVQA_Understanding_Data_CVPR_2018_paper.pdf") (Kafle et al., 2018)
        td #{45.77}

      tr
        td
          p #{"3"}
          span.date.label.label-default #{"March, 2020"}
        td
          | SAN <b>
          p.institution #{"Carnegie Mellon University"}
          a(href="https://arxiv.org/pdf/1511.02274.pdf") (Yang & al., '16)
        td #{32.10}


block content
  .cover#contentCover
    .container
      .row
        .col-md-5
          .infoCard
            .infoBody
              .infoHeadline
                h2 What is PlotQA?
              p 
                span 
                | PlotQA is a VQA dataset with 28.9 million question-answer pairs grounded over 224,377 plots on data from real-world sources and questions based on crowd-sourced question templates. 
              .infoHeadline
                h2 Why PlotQA?
              p 
                span 
                | Existing synthetic datasets (<a href="https://arxiv.org/pdf/1710.07300.pdf">FigureQA</a>, <a href="https://arxiv.org/pdf/1801.08163.pdf">DVQA</a>) for reasoning over plots do not contain variability in data labels, real-valued data, or complex reasoning questions. Consequently, proposed models for these datasets do not fully address the challenge of reasoning over plots. In particular, they assume that the answer comes either from a small fixed size vocabulary or from a bounding box within the image. However, in practice this is an unrealistic assumption because many questions require reasoning and thus have real valued answers which appear neither in a small fixed size vocabulary nor in the image. In this work, we aim to bridge this gap between existing datasets and real world plots by introducing PlotQA. Further, 80.76% of the out-of-vocabulary (OOV) questions in PlotQA have answers that are not in a fixed vocabulary.             
                <!-- a.btn.actionBtn(href="/explore/1.0/test/") Explore PlotQA -->
                a.btn.actionBtn(href="https://arxiv.org/pdf/1909.00997.pdf") PlotQA paper (WACV 2020)
              hr
              .infoHeadline
                h2 Getting Started
              p 
                | Download the dataset of 28.9 million question-answer pairs grounded over 224,377 plots. 
                ul.list-unstyled
                  li
                    a.btn.actionBtn(href="https://drive.google.com/drive/folders/1hCMwlZtxqu4-rxD5ja-lUUizOJfuNUfD?usp=sharing", download)
                      | Plot Images
                  li
                    a.btn.actionBtn(href="https://drive.google.com/drive/folders/1dwhZUp5vZQSRq0zanCacPB81kFhV6SfG?usp=sharing", download)
                      | Annotations
                  li
                    a.btn.actionBtn(href="https://drive.google.com/drive/folders/1Sh79vd9AgwnUk0QAzHMBxd1jXJW3KayQ?usp=sharing", download)
                      | QA Pairs
              hr
              .infoHeadline
                h2 PlotQA Pipeline
                p 
                span
                  | Our proposed pipeline (VOES) consists of various subtasks: (i) detect all the elements in the plot (bars, legend names, tick labels, etc), (ii) reads the values of these elements, (iii) establish relationship between the plot elements, and (iv) reason over this structured data.
                
                ul.list-unstyled
                  li
                    a.btn.actionBtn(href="https://github.com/NiteshMethani/PlotQA/blob/master/PlotQA_Pipeline.md", download)
                      | Download Code
                  li
              .infoHeadline
                h2 Have Questions?
              p 
                | Ask us questions at   
                a(href="mailto:nmethani@cse.iitm.ac.in") nmethani@cse.iitm.ac.in
                |  and 
                a(href="mailto:prithag@cse.iitm.ac.in") prithag@cse.iitm.ac.in
                | .
              .infoHeadline
                h2 Acknowledgements
              p 
                | Thank you <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD</a>
                | for allowing us to use the code to create this website.
            .infoSubheadline
              include includes/tweet
              include includes/github
        .col-md-7
          .infoCard
            .infoBody
              .infoHeadline
                h2 Results (trained and tested on PlotQA)
              p To assess the difficulty of the PlotQA dataset, we report human accuracy on a small subset of the Test split of the dataset. We also evaluate three state-of-the-art models on PlotQA and observe that uur proposed hybrid model significantly outperforms the existing models. It has an aggregate accuracy of 22.52% on the PlotQA dataset. We acknowledge that the accuracy is significantly lower than human performance. This establishes that the dataset is challenging and raises open questions on models for visual reasoning.
              +trained_on_random_negatives(test2, true)
          .infoCard
            .infoBody
              .infoHeadline
                h2 Results (trained and tested on DVQA)
              p 
                |  We evaluate our model on the test-set of DVQA. Our proposed hybrid model performs better than the existing models (SAN and SANY-OCR) establishing a new state-of-the-art result on DVQA.
              +trained_on_both(test1, true)
